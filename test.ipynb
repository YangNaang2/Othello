{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_games\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from DeepQResNet import DQN\n",
    "\n",
    "env = gym.make('Othello-v0',render_mode='human')\n",
    "state_shape = (8, 8, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward\n",
    "\n",
    "기본 : 자신의 색돌 이득 - 상대 색돌 이득\n",
    "게임 종료 보상 : +100/ -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Greedy 정책 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepQResNet import DQN\n",
    "import random\n",
    "import math\n",
    "env.metadata['autoplay'] = True\n",
    "env.metadata['render_fps'] = 1500000\n",
    "obs, reward, bdone, _, info = env.reset()\n",
    "dqn = DQN(state_shape,env.action_space.n)\n",
    "version = -1\n",
    "#dqn.load( f\"model/Greedy/BlackModel_{version}.weights.h5\")\n",
    "count = version +1\n",
    "def GetPolicy(bdone,wdone,turn,obs,actions):\n",
    "    if(not actions):\n",
    "        return None\n",
    "    if(turn==1):\n",
    "        if(bdone):\n",
    "            return None\n",
    "    else:\n",
    "        if(wdone):\n",
    "            return None\n",
    "    return dqn.EstimatePolicy(obs,turn,actions)\n",
    "def InsertBuffer(turn,oldobs, action, reward,obs,done,actions):\n",
    "    dqn.InsertBuffer(oldobs, action, reward,obs,done,actions,turn)\n",
    "\n",
    "for episode in range(1,400):\n",
    "    bdone = False\n",
    "    wdone = False\n",
    "    obs, reward, done, _, info = env.reset()\n",
    "    steps = 0\n",
    "    while (not bdone) or (not wdone):\n",
    "        actions = info['action']\n",
    "        turn = info['turn']\n",
    "        oldobs = obs\n",
    "        action = GetPolicy(bdone,wdone,turn,obs,actions)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        if done:\n",
    "            if info['turn'] ==2:\n",
    "                wdone = True\n",
    "            else:\n",
    "                bdone = True\n",
    "\n",
    "        if(action is not None):\n",
    "            InsertBuffer(turn,oldobs, action, reward,obs,done,info['action'])\n",
    "        steps += 1\n",
    "    loss = dqn.train()\n",
    "    if loss is not None:\n",
    "        print(f\"Episode {episode + 1},=Loss: {loss}\\n\")\n",
    "    if(episode % 30 == 0):\n",
    "        print(\"update Target Model\")\n",
    "        dqn.update_target_model()\n",
    "    if(episode % 100 == 0):\n",
    "        print('save \\n')\n",
    "        dqn.save(f\"model/Greedy/Model_{count}.weights.h5\")\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behavior 정책을 사용하여 학습 진행 \n",
    "\n",
    "UCT 활용\n",
    "<details>\n",
    "  <summary>Loss</summary>\n",
    "Episode 4, Loss: 5.625098705291748\n",
    "\n",
    "Episode 6, Loss: 5.289884567260742\n",
    "\n",
    "Episode 8, Loss: 4.940613269805908\n",
    "\n",
    "Episode 10, Loss: 4.973692893981934\n",
    "\n",
    "Episode 12, Loss: 4.738307952880859\n",
    "\n",
    "Episode 14, Loss: 4.937586307525635\n",
    "\n",
    "Episode 16, Loss: 4.8708648681640625\n",
    "\n",
    "Episode 19, Loss: 4.969433784484863\n",
    "\n",
    "Episode 21, Loss: 4.772389888763428\n",
    "\n",
    "Episode 23, Loss: 4.977059364318848\n",
    "\n",
    "Episode 25, Loss: 4.9247965812683105\n",
    "\n",
    "Episode 27, Loss: 4.974730968475342\n",
    "\n",
    "Episode 29, Loss: 4.74110746383667\n",
    "\n",
    "Episode 31, Loss: 4.632981777191162\n",
    "\n",
    "update Target Model\n",
    "Episode 34, Loss: 4.774975776672363\n",
    "\n",
    "Episode 36, Loss: 4.595297336578369\n",
    "\n",
    "Episode 38, Loss: 4.435949802398682\n",
    "\n",
    "Episode 40, Loss: 4.362507343292236\n",
    "\n",
    "Episode 42, Loss: 4.3570427894592285\n",
    "\n",
    "Episode 44, Loss: 4.358184814453125\n",
    "\n",
    "Episode 46, Loss: 4.297933101654053\n",
    "\n",
    "Episode 48, Loss: 4.297984600067139\n",
    "\n",
    "Episode 51, Loss: 4.351978302001953\n",
    "\n",
    "Episode 53, Loss: 4.346757411956787\n",
    "\n",
    "Episode 55, Loss: 4.296476364135742\n",
    "\n",
    "Episode 57, Loss: 4.297001361846924\n",
    "\n",
    "Episode 59, Loss: 4.292839050292969\n",
    "\n",
    "Episode 61, Loss: 4.248473167419434\n",
    "\n",
    "update Target Model\n",
    "Episode 63, Loss: 4.291788101196289\n",
    "\n",
    "Episode 66, Loss: 4.333674430847168\n",
    "\n",
    "Episode 68, Loss: 4.330304145812988\n",
    "\n",
    "Episode 70, Loss: 4.326761722564697\n",
    "\n",
    "Episode 72, Loss: 4.322862148284912\n",
    "\n",
    "Episode 74, Loss: 4.321232795715332\n",
    "\n",
    "Episode 76, Loss: 4.320329189300537\n",
    "\n",
    "Episode 78, Loss: 4.253968238830566\n",
    "\n",
    "Episode 81, Loss: 4.256453990936279\n",
    "\n",
    "Episode 83, Loss: 4.225350856781006\n",
    "\n",
    "Episode 85, Loss: 4.227616310119629\n",
    "\n",
    "Episode 87, Loss: 4.199090480804443\n",
    "\n",
    "Episode 89, Loss: 4.171319007873535\n",
    "\n",
    "Episode 91, Loss: 4.118231296539307\n",
    "\n",
    "update Target Model\n",
    "Episode 93, Loss: 4.12446928024292\n",
    "\n",
    "Episode 96, Loss: 4.127923011779785\n",
    "\n",
    "Episode 98, Loss: 4.1303324699401855\n",
    "\n",
    "Episode 100, Loss: 4.133883476257324\n",
    "\n",
    "save \n",
    "Episode 102, Loss: 4.136989116668701\n",
    "\n",
    "Episode 105, Loss: 4.190072536468506\n",
    "\n",
    "Episode 107, Loss: 4.1653008460998535\n",
    "\n",
    "Episode 109, Loss: 4.171037673950195\n",
    "\n",
    "Episode 111, Loss: 4.148536682128906\n",
    "\n",
    "Episode 113, Loss: 4.128968238830566\n",
    "\n",
    "Episode 115, Loss: 4.087134838104248\n",
    "\n",
    "Episode 118, Loss: 4.090423107147217\n",
    "\n",
    "Episode 120, Loss: 4.093200206756592\n",
    "\n",
    "update Target Model\n",
    "Episode 122, Loss: 4.139820575714111\n",
    "\n",
    "Episode 124, Loss: 4.140287399291992\n",
    "\n",
    "Episode 126, Loss: 4.12246561050415\n",
    "\n",
    "Episode 128, Loss: 4.1263251304626465\n",
    "\n",
    "Episode 130, Loss: 4.128334999084473\n",
    "\n",
    "Episode 133, Loss: 4.128984451293945\n",
    "\n",
    "Episode 135, Loss: 4.113494873046875\n",
    "\n",
    "Episode 137, Loss: 4.115747928619385\n",
    "\n",
    "Episode 139, Loss: 4.100518703460693\n",
    "\n",
    "Episode 141, Loss: 4.102965354919434\n",
    "\n",
    "Episode 143, Loss: 4.069893836975098\n",
    "\n",
    "Episode 145, Loss: 4.109523773193359\n",
    "\n",
    "Episode 148, Loss: 4.112292289733887\n",
    "\n",
    "Episode 150, Loss: 4.1138763427734375\n",
    "\n",
    "update Target Model\n",
    "Episode 152, Loss: 4.099748611450195\n",
    "\n",
    "Episode 154, Loss: 4.101950645446777\n",
    "\n",
    "Episode 156, Loss: 4.087657928466797\n",
    "\n",
    "Episode 158, Loss: 4.1077799797058105\n",
    "\n",
    "Episode 160, Loss: 4.109875679016113\n",
    "\n",
    "Episode 163, Loss: 4.161620616912842\n",
    "\n",
    "Episode 165, Loss: 4.178895950317383\n",
    "\n",
    "Episode 167, Loss: 4.196110725402832\n",
    "\n",
    "Episode 169, Loss: 4.166358470916748\n",
    "\n",
    "Episode 171, Loss: 4.168115139007568\n",
    "\n",
    "Episode 173, Loss: 4.168742656707764\n",
    "\n",
    "Episode 175, Loss: 4.1556925773620605\n",
    "\n",
    "Episode 178, Loss: 4.171774387359619\n",
    "\n",
    "Episode 181, Loss: 4.2165846824646\n",
    "\n",
    "update Target Model\n",
    "Episode 183, Loss: 4.232491970062256\n",
    "\n",
    "Episode 185, Loss: 4.218474864959717\n",
    "\n",
    "Episode 187, Loss: 4.23320198059082\n",
    "\n",
    "Episode 189, Loss: 4.219106197357178\n",
    "\n",
    "Episode 191, Loss: 4.233875751495361\n",
    "\n",
    "Episode 193, Loss: 4.2356438636779785\n",
    "\n",
    "Episode 195, Loss: 4.235965251922607\n",
    "\n",
    "Episode 198, Loss: 4.22313117980957\n",
    "\n",
    "Episode 200, Loss: 4.224108695983887\n",
    "\n",
    "save \n",
    "Episode 202, Loss: 4.21181583404541\n",
    "\n",
    "Episode 204, Loss: 4.213379383087158\n",
    "\n",
    "Episode 206, Loss: 4.202648162841797\n",
    "\n",
    "Episode 208, Loss: 4.204250812530518\n",
    "\n",
    "Episode 210, Loss: 4.2051682472229\n",
    "\n",
    "update Target Model\n",
    "Episode 213, Loss: 4.205852508544922\n",
    "\n",
    "Episode 215, Loss: 4.206500053405762\n",
    "\n",
    "Episode 217, Loss: 4.19531774520874\n",
    "\n",
    "Episode 219, Loss: 4.183105945587158\n",
    "\n",
    "Episode 221, Loss: 4.171559810638428\n",
    "\n",
    "Episode 223, Loss: 4.173558712005615\n",
    "\n",
    "Episode 225, Loss: 4.162966728210449\n",
    "\n",
    "Episode 227, Loss: 4.164514541625977\n",
    "\n",
    "Episode 230, Loss: 4.154328346252441\n",
    "\n",
    "Episode 232, Loss: 4.155423641204834\n",
    "\n",
    "Episode 234, Loss: 4.156314373016357\n",
    "\n",
    "Episode 236, Loss: 4.157158851623535\n",
    "\n",
    "Episode 238, Loss: 4.168328762054443\n",
    "\n",
    "Episode 240, Loss: 4.169682502746582\n",
    "\n",
    "update Target Model\n",
    "Episode 243, Loss: 4.170973300933838\n",
    "\n",
    "Episode 245, Loss: 4.160711288452148\n",
    "\n",
    "Episode 247, Loss: 4.151283264160156\n",
    "\n",
    "Episode 249, Loss: 4.163329124450684\n",
    "\n",
    "Episode 251, Loss: 4.164640426635742\n",
    "\n",
    "Episode 253, Loss: 4.187313556671143\n",
    "\n",
    "Episode 255, Loss: 4.178706169128418\n",
    "\n",
    "Episode 258, Loss: 4.1899285316467285\n",
    "\n",
    "Episode 260, Loss: 4.19043493270874\n",
    "\n",
    "Episode 262, Loss: 4.181771755218506\n",
    "\n",
    "Episode 264, Loss: 4.183167934417725\n",
    "\n",
    "Episode 266, Loss: 4.184128284454346\n",
    "\n",
    "Episode 268, Loss: 4.186351299285889\n",
    "\n",
    "Episode 270, Loss: 4.177640438079834\n",
    "\n",
    "update Target Model\n",
    "Episode 273, Loss: 4.179708480834961\n",
    "\n",
    "Episode 275, Loss: 4.1709113121032715\n",
    "\n",
    "Episode 277, Loss: 4.190140724182129\n",
    "\n",
    "Episode 279, Loss: 4.181586742401123\n",
    "\n",
    "Episode 281, Loss: 4.181912899017334\n",
    "\n",
    "Episode 283, Loss: 4.183043003082275\n",
    "\n",
    "Episode 285, Loss: 4.174756050109863\n",
    "\n",
    "Episode 287, Loss: 4.184872627258301\n",
    "\n",
    "Episode 290, Loss: 4.195497989654541\n",
    "\n",
    "Episode 292, Loss: 4.196559429168701\n",
    "\n",
    "Episode 294, Loss: 4.20595645904541\n",
    "\n",
    "Episode 296, Loss: 4.189056396484375\n",
    "\n",
    "Episode 298, Loss: 4.190039157867432\n",
    "\n",
    "Episode 300, Loss: 4.18238639831543\n",
    "\n",
    "update Target Model\n",
    "save \n",
    "Episode 302, Loss: 4.182745933532715\n",
    "\n",
    "Episode 305, Loss: 4.20911979675293\n",
    "\n",
    "Episode 307, Loss: 4.1921539306640625\n",
    "\n",
    "Episode 309, Loss: 4.184631824493408\n",
    "\n",
    "Episode 311, Loss: 4.185292720794678\n",
    "\n",
    "Episode 313, Loss: 4.210738182067871\n",
    "\n",
    "Episode 315, Loss: 4.2022809982299805\n",
    "\n",
    "Episode 317, Loss: 4.195312976837158\n",
    "\n",
    "Episode 319, Loss: 4.212935447692871\n",
    "\n",
    "Episode 322, Loss: 4.213899612426758\n",
    "\n",
    "Episode 324, Loss: 4.213501930236816\n",
    "\n",
    "Episode 326, Loss: 4.206076145172119\n",
    "\n",
    "Episode 328, Loss: 4.191421031951904\n",
    "\n",
    "Episode 330, Loss: 4.192600250244141\n",
    "\n",
    "update Target Model\n",
    "Episode 332, Loss: 4.185030460357666\n",
    "\n",
    "Episode 334, Loss: 4.186481952667236\n",
    "\n",
    "Episode 337, Loss: 4.21052360534668\n",
    "\n",
    "Episode 339, Loss: 4.202536582946777\n",
    "\n",
    "Episode 341, Loss: 4.203289985656738\n",
    "\n",
    "Episode 343, Loss: 4.211679935455322\n",
    "\n",
    "Episode 345, Loss: 4.197455883026123\n",
    "\n",
    "Episode 347, Loss: 4.190333843231201\n",
    "\n",
    "Episode 349, Loss: 4.191042900085449\n",
    "\n",
    "Episode 351, Loss: 4.184715747833252\n",
    "\n",
    "Episode 354, Loss: 4.193368911743164\n",
    "\n",
    "Episode 356, Loss: 4.194478988647461\n",
    "\n",
    "Episode 358, Loss: 4.202226161956787\n",
    "\n",
    "Episode 360, Loss: 4.203236103057861\n",
    "\n",
    "update Target Model\n",
    "Episode 362, Loss: 4.196558475494385\n",
    "\n",
    "Episode 364, Loss: 4.18941593170166\n",
    "\n",
    "Episode 366, Loss: 4.1908745765686035\n",
    "\n",
    "Episode 369, Loss: 4.205924987792969\n",
    "\n",
    "Episode 371, Loss: 4.2064056396484375\n",
    "\n",
    "Episode 373, Loss: 4.199900150299072\n",
    "\n",
    "Episode 375, Loss: 4.199772834777832\n",
    "\n",
    "Episode 377, Loss: 4.193443298339844\n",
    "\n",
    "Episode 379, Loss: 4.194346904754639\n",
    "\n",
    "Episode 381, Loss: 4.188399791717529\n",
    "\n",
    "Episode 384, Loss: 4.182060718536377\n",
    "\n",
    "Episode 386, Loss: 4.182889938354492\n",
    "\n",
    "Episode 388, Loss: 4.190266132354736\n",
    "\n",
    "Episode 390, Loss: 4.1905388832092285\n",
    "\n",
    "update Target Model\n",
    "Episode 392, Loss: 4.191070079803467\n",
    "\n",
    "Episode 394, Loss: 4.184800148010254\n",
    "\n",
    "Episode 396, Loss: 4.178762435913086\n",
    "\n",
    "Episode 398, Loss: 4.192815780639648\n",
    "\n",
    "Episode 401, Loss: 4.23275899887085\n",
    "\n",
    "save \n",
    "Episode 403, Loss: 4.226405143737793\n",
    "\n",
    "Episode 405, Loss: 4.220861434936523\n",
    "\n",
    "Episode 407, Loss: 4.221346855163574\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepQResNet import DQN\n",
    "import random\n",
    "env.metadata['autoplay'] = True\n",
    "env.metadata['render_fps'] = 1500000\n",
    "obs, reward, bdone, _, info = env.reset()\n",
    "\n",
    "dqn = DQN(state_shape,env.action_space.n)\n",
    "version = -1\n",
    "#dqn.load( f\"model/UCT/Model_{version}.weights.h5\")\n",
    "Count = version +1\n",
    "def GetPolicy(bdone,wdone,turn,env,obs,actions):\n",
    "    if(not actions):\n",
    "        return None\n",
    "    if(turn==1):\n",
    "        if(bdone):\n",
    "            return None\n",
    "    else:\n",
    "        if(wdone):\n",
    "            return None\n",
    "    return dqn.BehaviorPolicy(env,obs,turn,actions)\n",
    "def InsertBuffer(turn,oldobs, action, reward,obs,done,actions):\n",
    "    dqn.InsertBuffer(oldobs, action, reward,obs,done,actions,turn)\n",
    "for episode in range(1,400):\n",
    "    bdone = False\n",
    "    wdone = False\n",
    "    obs, reward, done, _, info = env.reset()\n",
    "    steps = 0\n",
    "    while (not bdone) or (not wdone):\n",
    "        actions = info['action']\n",
    "        turn = info['turn']\n",
    "        oldobs = obs\n",
    "        action = GetPolicy(bdone,wdone,turn,env,obs,actions)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        if done:\n",
    "            if info['turn'] ==2:\n",
    "                wdone = True\n",
    "            else:\n",
    "                bdone = True\n",
    "        if(action is not None):\n",
    "            InsertBuffer(turn,oldobs, action, reward,obs,done,actions)\n",
    "        steps += 1\n",
    "    loss = dqn.train()\n",
    "    if loss is not None:\n",
    "        print(f\"Episode {episode + 1}, Loss: {loss}\\n\")\n",
    "    if(episode % 30 == 0):\n",
    "        print(\"update Target Model\")\n",
    "        dqn.update_target_model()\n",
    "    if(episode % 100 == 0):\n",
    "        print('save ')\n",
    "        dqn.save(f\"model/UCT/Model_{Count}.weights.h5\")\n",
    "        Count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4, BlackLoss: 1.614793300628662\n",
      "\n",
      "Episode 4, WhiteLoss: 1.7577934265136719\n",
      "\n",
      "Episode 6, BlackLoss: 1.349267601966858\n",
      "\n",
      "Episode 6, WhiteLoss: 1.512926697731018\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001534307C2C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001534037C040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Episode 8, BlackLoss: 1.1248804330825806\n",
      "\n",
      "Episode 8, WhiteLoss: 1.2767577171325684\n",
      "\n",
      "Episode 10, BlackLoss: 0.9338687658309937\n",
      "\n",
      "Episode 10, WhiteLoss: 1.0718914270401\n",
      "\n",
      "Episode 12, BlackLoss: 0.7854718565940857\n",
      "\n",
      "Episode 12, WhiteLoss: 0.9076473116874695\n",
      "\n",
      "Episode 14, BlackLoss: 0.6724572777748108\n",
      "\n",
      "Episode 14, WhiteLoss: 0.7806628346443176\n",
      "\n",
      "Episode 16, BlackLoss: 0.5856212973594666\n",
      "\n",
      "Episode 16, WhiteLoss: 0.6815481781959534\n",
      "\n",
      "Episode 19, BlackLoss: 0.5188606977462769\n",
      "\n",
      "Episode 19, WhiteLoss: 0.6032830476760864\n",
      "\n",
      "Episode 21, BlackLoss: 0.46567270159721375\n",
      "\n",
      "Episode 21, WhiteLoss: 0.5406937003135681\n",
      "\n",
      "Episode 23, BlackLoss: 0.4225471615791321\n",
      "\n",
      "Episode 23, WhiteLoss: 0.4898558557033539\n",
      "\n",
      "Episode 25, BlackLoss: 0.3868508040904999\n",
      "\n",
      "Episode 25, WhiteLoss: 0.4479089677333832\n",
      "\n",
      "Episode 27, BlackLoss: 0.35697707533836365\n",
      "\n",
      "Episode 27, WhiteLoss: 0.4129396677017212\n",
      "\n",
      "Episode 29, BlackLoss: 0.33184462785720825\n",
      "\n",
      "Episode 29, WhiteLoss: 0.38327041268348694\n",
      "\n",
      "Episode 31, BlackLoss: 0.30983591079711914\n",
      "\n",
      "Episode 31, WhiteLoss: 0.35805264115333557\n",
      "\n",
      "update Target Model\n",
      "Episode 34, BlackLoss: 0.29084286093711853\n",
      "\n",
      "Episode 34, WhiteLoss: 0.3359762132167816\n",
      "\n",
      "Episode 36, BlackLoss: 0.2741750478744507\n",
      "\n",
      "Episode 36, WhiteLoss: 0.31666168570518494\n",
      "\n",
      "Episode 38, BlackLoss: 0.25953805446624756\n",
      "\n",
      "Episode 38, WhiteLoss: 0.299524188041687\n",
      "\n",
      "Episode 40, BlackLoss: 0.2466568946838379\n",
      "\n",
      "Episode 40, WhiteLoss: 0.28410059213638306\n",
      "\n",
      "Episode 42, BlackLoss: 0.23502106964588165\n",
      "\n",
      "Episode 42, WhiteLoss: 0.27015963196754456\n",
      "\n",
      "Episode 44, BlackLoss: 0.2245093137025833\n",
      "\n",
      "Episode 44, WhiteLoss: 0.2575133144855499\n",
      "\n",
      "Episode 46, BlackLoss: 0.21506278216838837\n",
      "\n",
      "Episode 46, WhiteLoss: 0.24595721065998077\n",
      "\n",
      "Episode 48, BlackLoss: 0.2066146284341812\n",
      "\n",
      "Episode 48, WhiteLoss: 0.23532269895076752\n",
      "\n",
      "Episode 51, BlackLoss: 0.19902701675891876\n",
      "\n",
      "Episode 51, WhiteLoss: 0.22554995119571686\n",
      "\n",
      "Episode 53, BlackLoss: 0.19198483228683472\n",
      "\n",
      "Episode 53, WhiteLoss: 0.21656382083892822\n",
      "\n",
      "Episode 55, BlackLoss: 0.18581824004650116\n",
      "\n",
      "Episode 55, WhiteLoss: 0.20830707252025604\n",
      "\n",
      "Episode 57, BlackLoss: 0.18015091121196747\n",
      "\n",
      "Episode 57, WhiteLoss: 0.20062778890132904\n",
      "\n",
      "Episode 59, BlackLoss: 0.17497625946998596\n",
      "\n",
      "Episode 59, WhiteLoss: 0.1935187131166458\n",
      "\n",
      "Episode 61, BlackLoss: 0.16997189819812775\n",
      "\n",
      "Episode 61, WhiteLoss: 0.1868899166584015\n",
      "\n",
      "update Target Model\n",
      "Episode 63, BlackLoss: 0.1654573380947113\n",
      "\n",
      "Episode 63, WhiteLoss: 0.18074741959571838\n",
      "\n",
      "Episode 66, BlackLoss: 0.16164012253284454\n",
      "\n",
      "Episode 66, WhiteLoss: 0.1750335991382599\n",
      "\n",
      "Episode 68, BlackLoss: 0.15809020400047302\n",
      "\n",
      "Episode 68, WhiteLoss: 0.16969558596611023\n",
      "\n",
      "Episode 70, BlackLoss: 0.15475541353225708\n",
      "\n",
      "Episode 70, WhiteLoss: 0.16471076011657715\n",
      "\n",
      "Episode 72, BlackLoss: 0.1515662670135498\n",
      "\n",
      "Episode 72, WhiteLoss: 0.16011008620262146\n",
      "\n",
      "Episode 74, BlackLoss: 0.14864306151866913\n",
      "\n",
      "Episode 74, WhiteLoss: 0.15576615929603577\n",
      "\n",
      "Episode 76, BlackLoss: 0.14566722512245178\n",
      "\n",
      "Episode 76, WhiteLoss: 0.15160562098026276\n",
      "\n",
      "Episode 78, BlackLoss: 0.14294075965881348\n",
      "\n",
      "Episode 78, WhiteLoss: 0.1477113664150238\n",
      "\n",
      "Episode 80, BlackLoss: 0.13998061418533325\n",
      "\n",
      "Episode 80, WhiteLoss: 0.14398150146007538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def GetPolicy(bdone,wdone,turn,env,obs,actions):\n",
    "    if(not actions):\n",
    "        return None\n",
    "    if(turn==1):\n",
    "        if(bdone):\n",
    "            return None\n",
    "        return Blackdqn.BehaviorPolicy(env,obs,turn,actions)\n",
    "    else:\n",
    "        if(wdone):\n",
    "            return None\n",
    "        return Whitedqn.BehaviorPolicy(env,obs,turn,actions)\n",
    "def InsertBuffer(turn,oldobs, action, reward,obs,done,actions):\n",
    "    Blackdqn.InsertBuffer(oldobs, action, reward,obs,done,actions,turn)\n",
    "    Whitedqn.InsertBuffer(oldobs, action, reward,obs,done,actions,turn)\n",
    "\n",
    "env.metadata['autoplay'] = True\n",
    "env.metadata['render_fps'] = 150000\n",
    "obs, reward, bdone, _, info = env.reset()\n",
    "state_shape = (8, 8, 1)  # Adding the channel dimension\n",
    "Blackdqn = DQN(state_shape,env.action_space.n)\n",
    "Whitedqn = DQN(state_shape,env.action_space.n)\n",
    "\n",
    "version = -1\n",
    "# Blackdqn.load( f\"model/UCT/BlackModel_{version}.weights.h5\")\n",
    "# Whitedqn.load(f\"model/UCT/WhiteModel_{version}.weights.h5\")\n",
    "BlackmodelCount = version +1\n",
    "WhitemodelCount = version +1\n",
    "\n",
    "for episode in range(1,10000):\n",
    "    bdone = False\n",
    "    wdone = False\n",
    "    obs, reward, done, _, info = env.reset()\n",
    "    steps = 0\n",
    "    while (not bdone) or (not wdone):\n",
    "        actions = info['action']\n",
    "        turn = info['turn']\n",
    "        oldobs = obs\n",
    "        action = GetPolicy(bdone,wdone,turn,env,obs,actions)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        if done:\n",
    "            if info['turn'] ==2:\n",
    "                wdone = True\n",
    "            else:\n",
    "                bdone = True\n",
    "        if(action is not None):\n",
    "            InsertBuffer(turn,oldobs, action, reward,obs,done,actions)\n",
    "        steps += 1\n",
    "    blakloss = Blackdqn.train()\n",
    "    whiteloss = Whitedqn.train()\n",
    "    if blakloss is not None:\n",
    "        print(f\"Episode {episode + 1}, BlackLoss: {blakloss}\\n\")\n",
    "        Blackdqn.update_target_model()\n",
    "    if whiteloss is not None:\n",
    "        print(f\"Episode {episode + 1}, WhiteLoss: {whiteloss}\\n\")\n",
    "        Whitedqn.update_target_model()\n",
    "    if(episode % 30 == 0):\n",
    "        print(\"update Target Model\")\n",
    "        Blackdqn.update_target_model()\n",
    "        Whitedqn.update_target_model()\n",
    "    if(episode % 100 == 0):\n",
    "        print('save ')\n",
    "        Whitedqn.save(f\"model/WhiteModel_{WhitemodelCount}.weights.h5\")\n",
    "        Blackdqn.save(f\"model/BlackModel_{BlackmodelCount}.weights.h5\")\n",
    "        BlackmodelCount+=1\n",
    "        WhitemodelCount+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사람  vs 컴퓨터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "    \n",
    "env.metadata['render_fps'] = 60\n",
    "dqn = DQN(state_shape,env.action_space.n)\n",
    "\n",
    "\n",
    "#dqn.load('model/Greedy/Model_2.weights.h5')\n",
    "dqn.load('model/UCT/Black/BlackModel_20.weights.h5')\n",
    "#dqn.load('model/UCT/white/WhiteModel_1.weights.h5')\n",
    "\n",
    "bdone = False\n",
    "wdone = False\n",
    "BlackPlay = False # 사람이 무슨 색 돌로 시작할 건지\n",
    "\n",
    "env.metadata['autoplay'] = not BlackPlay\n",
    "obs, reward, done, _, info = env.reset()\n",
    "\n",
    "def GetPolicy(bdone,wdone,turn,env,obs,actions):\n",
    "    if(not actions):\n",
    "        return None\n",
    "    if(turn==1):\n",
    "        if(bdone):\n",
    "            return None\n",
    "    else:\n",
    "        if(wdone):\n",
    "            return None\n",
    "    if(not env.metadata['autoplay']):\n",
    "        return None\n",
    "    return dqn.EstimatePolicy(obs,turn,actions)\n",
    "\n",
    "while (not bdone) or (not wdone):\n",
    "    actions = info['action']\n",
    "    turn = info['turn']\n",
    "    oldobs = obs\n",
    "    time.sleep(0.5)\n",
    "    action = GetPolicy(bdone,wdone,turn,env,obs,actions)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    env.metadata['autoplay'] = not env.metadata['autoplay']\n",
    "    env.render()\n",
    "    if done:\n",
    "        if info['turn'] ==2:\n",
    "            wdone = True\n",
    "        else:\n",
    "            bdone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
